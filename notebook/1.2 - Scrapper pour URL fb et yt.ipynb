{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapper pour obtenir les données sur les URLS facebook et Youtube"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### importation des modules custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tousi\\Google Drive\\radio_canada\\projet\\decrypteur\\repo_pyMboxAnalyser\\analyser_tools\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..')) +'\\\\analyser_tools'\n",
    "print(module_path)\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from EmailDataFrame import *\n",
    "from FacebookScrapper import FacebookScrapper\n",
    "from YoutubeScrapper import YoutubeScrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import python modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### definir les paths vers les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = pathlib.PurePath(os.getcwd())\n",
    "project_root_dir = cwd.parent\n",
    "theme_path = project_root_dir / \"data\" / \"theme_words.csv\"\n",
    "email_raw_path = project_root_dir / \"output\" / 'csv_file' / 'email_raw.csv'\n",
    "facebook_url_info_path = project_root_dir / 'output' / 'csv_file' / 'facebook_urls_info.csv'\n",
    "youtube_url_info_path = project_root_dir / 'output' / 'csv_file' / 'youtube_urls_info.csv'\n",
    "log_file_path = project_root_dir / 'output' / 'log_file' / 'url_cleaning.log'\n",
    "removed_email_path = project_root_dir / 'output' / 'csv_file' / 'removed_email_from_decrypteur.csv'\n",
    "dataclean_pickle_path = project_root_dir / 'output' / 'pickle_obj' / 'dataClean'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|███████████████████████████████████████▍                                   | 2753/5230 [00:00<00:00, 27330.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating domain column\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 5230/5230 [00:00<00:00, 26891.48it/s]\n",
      "  2%|█▌                                                                           | 103/5230 [00:00<00:05, 1022.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating has_url column\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 5230/5230 [00:04<00:00, 1117.64it/s]\n",
      "  1%|▍                                                                              | 33/5230 [00:00<00:16, 324.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating text column\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 5230/5230 [00:12<00:00, 425.37it/s]\n",
      "  0%|▏                                                                              | 12/5230 [00:00<00:48, 108.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tagging theme to email\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 5230/5230 [00:27<00:00, 187.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tagging source to email\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "email_raw_converters = {\"attach_type\": lambda x: x.strip(\"[]\").replace(\"'\", \"\").lower().strip().split(\", \"),\n",
    "                            \"urls\": lambda x: x.strip(\"[]\").replace(\"'\", \"\").strip().split(', ')\n",
    "                            }\n",
    "email_df_raw = EmailDF.from_csv(csv_path=email_raw_path, converters=email_raw_converters)\n",
    "email_df_raw.read_dict_theme_from_csv(theme_path)\n",
    "\n",
    "debut = (2020, 3, 1) # 1er mars 2020 pour le premier courriel\n",
    "fin = (9999, 12, 30) # infini\n",
    "\n",
    "email_df = (email_df_raw.filt_by_date(start_date=debut, end_date=fin) #conserver entre debut et fin\n",
    "            .remove_email_by_from(logpath=removed_email_path) #log les courriels retirer \n",
    "            .clean_urls() #courriel spam (antivirus), repetition de domaine\n",
    "            .add_domain_column() #trouver domaine des urls restants\n",
    "            .add_has_url_column() #ajouter s'il y a des urls ou non\n",
    "            .add_text_column() # combiner body et titre du courriel, puis retirer emoji, non-roman text, saut de ligne et tab\n",
    "            .add_theme_column() # tagger les courriels à partir du text\n",
    "            .add_source_column()) # trouver les sources (comme domaine, mais sans répétition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrapper "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/872 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing facebook urls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 872/872 [10:24<00:00,  1.40it/s]\n",
      "  0%|                                                                                          | 0/542 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing youtube urls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 542/542 [44:18<00:00,  4.90s/it]\n"
     ]
    }
   ],
   "source": [
    "print('parsing facebook urls')\n",
    "facebook_urls = email_df.df.explode('urls')[email_df.df.domain.explode() == 'facebook']['urls'].value_counts()\n",
    "data = []\n",
    "for url in tqdm(facebook_urls.index):\n",
    "    fs = FacebookScrapper(url, email_df.theme)\n",
    "    fs.parse_data()\n",
    "    data.append(fs.export_as_list())\n",
    "df_urls_facebook = pd.DataFrame(data, columns = ['url', 'title', 'published_date', 'comments', 'reactions', 'shares', 'views', 'theme'])\n",
    "df_urls_facebook.to_csv(facebook_url_info_path)\n",
    "\n",
    "\n",
    "print('parsing youtube urls')\n",
    "youtube_urls = email_df.df.explode('urls')[email_df.df.domain.explode() == 'youtube']['urls'].value_counts()\n",
    "data = []\n",
    "for url in tqdm(youtube_urls.index):\n",
    "    fs = YoutubeScrapper(url, email_df.theme)\n",
    "    fs.parse_data()\n",
    "    data.append(fs.export_to_list())\n",
    "df_urls_youtube = pd.DataFrame(data, columns = ['url', 'title', 'date_published', 'description', 'author', 'rating', 'views', 'theme'])\n",
    "df_urls_youtube.to_csv(youtube_url_info_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
